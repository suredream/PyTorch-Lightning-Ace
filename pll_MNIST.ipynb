{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pll-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17YT5137RFzVl9Y8yhXPP0MeyZNq-alli",
      "authorship_tag": "ABX9TyP1bCL9dPOq7T+97cxFOg+L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suredream/PyTorch-Lightning-Ace/blob/main/pll_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/utils')\n",
        "%reload_ext git"
      ],
      "metadata": {
        "id": "ZhXL9gEPKhBV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%git clone PyTorch-Lightning-Ace"
      ],
      "metadata": {
        "id": "8zjBZXzkLPFn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVyO0YjdM_pY",
        "outputId": "01c8910c-031d-43b1-fd1a-aedae8f478c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%git push"
      ],
      "metadata": {
        "id": "V8exkPWpMZUA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pytorch_lightning\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNWU5kG9EsB2",
        "outputId": "563b8a59-8d92-420a-b489-b4f621a5d575"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 585 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 54.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 807 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn import *\n",
        "from torch.optim import *\n",
        "from torch.optim.lr_scheduler import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "import os, gc, time\n",
        "import math, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import *"
      ],
      "metadata": {
        "id": "-CKkbrHq_SgK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc54OvrKEaTH",
        "outputId": "e1c3ad38-debf-43ce-c875-4181ab95a7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name            | Type             | Params\n",
            "-----------------------------------------------------\n",
            "0 | conv1           | Conv2d           | 260   \n",
            "1 | conv2           | Conv2d           | 1.8 K \n",
            "2 | fc1             | Linear           | 1.0 M \n",
            "3 | fc2             | Linear           | 5.0 K \n",
            "4 | train_criterion | CrossEntropyLoss | 0     \n",
            "5 | val_criterion   | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------------\n",
            "1.0 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.0 M     Total params\n",
            "4.030     Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   epoch    |    time    | loss/train | acc/train  |  f1/train  | loss/test  |  acc/test  |  f1/test   |\n",
            "|------------|------------|------------|------------|------------|------------|------------|------------|\n",
            "|     0      |     12     |  0.171176  |  0.950067  |  0.949726  |  0.054144  |  0.981200  |  0.981170  |\n",
            "|     1      |     13     |  0.048940  |  0.984433  |  0.984332  |  0.037225  |  0.987200  |  0.987089  |\n",
            "|     2      |     13     |  0.031853  |  0.990000  |  0.989965  |  0.034371  |  0.988600  |  0.988467  |\n",
            "|     3      |     13     |  0.022517  |  0.992767  |  0.992731  |  0.038290  |  0.987100  |  0.987096  |\n",
            "|     4      |     13     |  0.017995  |  0.994233  |  0.994212  |  0.035006  |  0.988400  |  0.988376  |\n",
            "|     5      |     13     |  0.011745  |  0.996183  |  0.996173  |  0.034702  |  0.990400  |  0.990312  |\n",
            "|     6      |     13     |  0.011421  |  0.996200  |  0.996188  |  0.043786  |  0.987800  |  0.987702  |\n",
            "|     7      |     13     |  0.009266  |  0.997083  |  0.997075  |  0.044331  |  0.988100  |  0.987973  |\n",
            "|     8      |     13     |  0.007245  |  0.997467  |  0.997450  |  0.039357  |  0.990300  |  0.990196  |\n",
            "|     9      |     12     |  0.006809  |  0.997700  |  0.997713  |  0.044875  |  0.989600  |  0.989522  |\n"
          ]
        }
      ],
      "source": [
        "layout = {\n",
        "    \"results\": {\n",
        "        \"loss\": [\"Multiline\", [\"loss/train\", \"loss/test\"]],\n",
        "        \"accuracy\": [\"Multiline\", [\"acc/train\", \"acc/test\"]],\n",
        "        \"f1\": [\"Multiline\", [\"f1/train\", \"f1/test\"]],\n",
        "    },\n",
        "}\n",
        "writer = SummaryWriter()\n",
        "writer.add_custom_scalars(layout)\n",
        "\n",
        "# 定义超参数\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "\n",
        "# 创建数据集和生成器\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=train_transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataset = datasets.MNIST(\"data\", train=False, transform=val_transform)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# 定义损失函数和指标\n",
        "\n",
        "\n",
        "class CrossEntropyLoss(Module): # 和Pytorch官方实现一样\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target, reduction=\"mean\"):\n",
        "        N, C = input.size()\n",
        "\n",
        "        if target.dim() > 1:\n",
        "            one_hot = target\n",
        "        else:\n",
        "            one_hot = torch.zeros((N, C), dtype=input.dtype, device=input.device)\n",
        "            one_hot.scatter_(1, target.reshape(N, 1), 1)\n",
        "\n",
        "        loss = -(one_hot * F.log_softmax(input, 1)).sum(1)\n",
        "        if reduction == \"mean\":\n",
        "            return loss.mean(0)\n",
        "        elif reduction == \"sum\":\n",
        "            return loss.sum(0)\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "class ClassificationMetric(object): # 记录结果并计算指标\n",
        "    def __init__(self, accuracy=True, recall=True, precision=True, f1=True, average=\"macro\"):\n",
        "        self.accuracy = accuracy\n",
        "        self.recall = recall\n",
        "        self.precision = precision\n",
        "        self.f1 = f1\n",
        "        self.average = average\n",
        "\n",
        "        self.preds = []\n",
        "        self.target = []\n",
        "\n",
        "    def reset(self): # 重置结果\n",
        "        self.preds.clear()\n",
        "        self.target.clear()\n",
        "        gc.collect()\n",
        "\n",
        "    def update(self, preds, target): # 更新结果\n",
        "        preds = list(preds.cpu().detach().argmax(1).numpy())\n",
        "        target = list(target.cpu().detach().argmax(1).numpy()) if target.dim() > 1 else list(target.cpu().detach().numpy())\n",
        "        self.preds += preds\n",
        "        self.target += target\n",
        "\n",
        "    def compute(self): # 计算结果\n",
        "        metrics = []\n",
        "        if self.accuracy:\n",
        "            metrics.append(accuracy_score(self.target, self.preds))\n",
        "        if self.recall:\n",
        "            metrics.append(recall_score(self.target, self.preds, labels=list(set(self.preds)), average=self.average))\n",
        "        if self.precision:\n",
        "            metrics.append(precision_score(self.target, self.preds, labels=list(set(self.preds)), average=self.average))\n",
        "        if self.f1:\n",
        "            metrics.append(f1_score(self.target, self.preds, labels=list(set(self.preds)), average=self.average))\n",
        "        self.reset()\n",
        "        return metrics\n",
        "\n",
        "\n",
        "# 定义模型和训练逻辑\n",
        "\n",
        "\n",
        "class CustomModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 定义网络层\n",
        "        self.conv1 = Conv2d(1, 10, 5)\n",
        "        self.conv2 = Conv2d(10, 20, 3)\n",
        "        self.fc1 = Linear(20*10*10, 500)\n",
        "        self.fc2 = Linear(500, 10)\n",
        "        # 定义损失函数\n",
        "        self.train_criterion = CrossEntropyLoss()\n",
        "        self.val_criterion = CrossEntropyLoss()\n",
        "        # 定义指标\n",
        "        self.train_metric = ClassificationMetric(recall=False, precision=False)\n",
        "        self.val_metric = ClassificationMetric(recall=False, precision=False)\n",
        "        # 定义log\n",
        "        self.history = {\n",
        "            \"loss/train\": [], \"acc/train\": [], \"f1/train\": [],\n",
        "            \"loss/test\": [], \"acc/test\": [], \"f1/test\": [],\n",
        "        }\n",
        "        \n",
        "    def forward(self,x):\n",
        "        in_size = x.size(0)\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(out)\n",
        "        out = F.max_pool2d(out, 2, 2)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(out)\n",
        "        out = out.view(in_size, -1)\n",
        "        out = self.fc1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = F.log_softmax(out, dim=1)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch, idx):\n",
        "        x, y = batch\n",
        "        _y = self(x)\n",
        "        # 计算loss\n",
        "        loss = self.train_criterion(_y, y)\n",
        "        # 更新结果\n",
        "        self.train_metric.update(_y, y)\n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outs):\n",
        "        # 计算平均loss\n",
        "        loss = 0.\n",
        "        for out in outs:\n",
        "            loss += out[\"loss\"].cpu().detach().item()\n",
        "        loss /= len(outs)\n",
        "        # 计算指标\n",
        "        acc, f1 = self.train_metric.compute()\n",
        "        # 记录log\n",
        "        self.history[\"loss/train\"].append(loss)\n",
        "        self.history[\"acc/train\"].append(acc)\n",
        "        self.history[\"f1/train\"].append(f1)\n",
        "\n",
        "    def validation_step(self, batch, idx):\n",
        "        x, y = batch\n",
        "        _y = self(x)\n",
        "        val_loss = self.val_criterion(_y, y)\n",
        "        self.val_metric.update(_y, y)\n",
        "        return val_loss\n",
        "\n",
        "    def validation_epoch_end(self, outs):\n",
        "        val_loss = sum(outs).item() / len(outs)\n",
        "        val_acc, val_f1 = self.val_metric.compute()\n",
        "\n",
        "        self.history[\"loss/test\"].append(val_loss)\n",
        "        self.history[\"acc/test\"].append(val_acc)\n",
        "        self.history[\"f1/test\"].append(val_f1)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # 设置优化器\n",
        "        return Adam(self.parameters())\n",
        "\n",
        "model = CustomModel()\n",
        "\n",
        "\n",
        "# 定义进度条和学习曲线\n",
        "class FlexibleTqdm(Callback):\n",
        "    def __init__(self, steps, column_width=10):\n",
        "        super(FlexibleTqdm, self).__init__()\n",
        "        self.steps = steps\n",
        "        self.column_width = column_width\n",
        "        self.info = \"\\rEpoch_%d %s%% [%s]\"\n",
        "\n",
        "    def on_train_start(self, trainer, module):\n",
        "        history = module.history\n",
        "        title, row = \"|\", \"|\"\n",
        "        for i in ['epoch','time'] + list(history.keys()):\n",
        "            title += i.center(self.column_width) + \"|\"\n",
        "            row += \"-\" * (self.column_width) + \"|\"\n",
        "        print(title)\n",
        "        print(row)\n",
        "\n",
        "    def on_train_batch_end(self, trainer, module, outputs, batch, batch_idx):\n",
        "        current_index = int((batch_idx + 1) * 100 / self.steps)\n",
        "        tqdm = [\".\"] * 100\n",
        "        for i in range(current_index - 1):\n",
        "            tqdm[i] = \"=\"\n",
        "        if current_index:\n",
        "            tqdm[current_index - 1] = \">\"\n",
        "        print(self.info % (module.current_epoch, str(current_index).rjust(3), \"\".join(tqdm)), end=\"\")\n",
        "\n",
        "    def on_train_epoch_start(self, trainer, module):\n",
        "        print(self.info % (module.current_epoch, \"  0\", \".\" * 100), end=\"\")\n",
        "        self.begin = time.perf_counter()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, module):\n",
        "        self.end = time.perf_counter()\n",
        "        history = module.history\n",
        "        detail = \"\\r|\"\n",
        "        detail += str(module.current_epoch).center(self.column_width) + \"|\"\n",
        "        detail += (\"%d\" % (self.end - self.begin)).center(self.column_width) + \"|\"\n",
        "        for k, j in history.items():#.values():\n",
        "            if j:\n",
        "                detail += (\"%.06f\" % j[-1]).center(self.column_width) + \"|\"\n",
        "                writer.add_scalar(k, j[-1], module.current_epoch)\n",
        "        print(\"\\r\" + \" \" * 120, end=\"\")\n",
        "        print(detail)\n",
        "        \n",
        "        \n",
        "class LearningCurve(Callback):\n",
        "    def __init__(self, figsize=(12, 4), names=(\"loss\", \"acc\", \"f1\")):\n",
        "        super(LearningCurve, self).__init__()\n",
        "        self.figsize = figsize\n",
        "        self.names = names\n",
        "\n",
        "    def on_fit_end(self, trainer, module):\n",
        "        history = module.history\n",
        "        plt.figure(figsize=self.figsize)\n",
        "        for i, j in enumerate(self.names):\n",
        "            plt.subplot(1, len(self.names), i + 1)\n",
        "            plt.title(j + \"/val_\" + j)\n",
        "            plt.plot(history[j], \"--o\", color='r', label=j)\n",
        "            plt.plot(history[\"val_\" + j], \"-*\", color='g', label=\"val_\" + j)\n",
        "            plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# 设置训练参数（我不喜欢用logger，所以全关闭了）\n",
        "\n",
        "\n",
        "trainer_params = {\n",
        "    \"gpus\": 1,\n",
        "    \"max_epochs\": epochs,  # 1000\n",
        "    \"enable_checkpointing\": False,  # True\n",
        "    \"logger\": False,  # TensorBoardLogger\n",
        "    \"enable_progress_bar\": False,  # 1\n",
        "    \"num_sanity_val_steps\": 0,  # 2\n",
        "    \"callbacks\": [\n",
        "        FlexibleTqdm(len(train_dataset) // batch_size, column_width=12), # 注意设置progress_bar_refresh_rate=0，取消自带的进度条\n",
        "        # LearningCurve(figsize=(12, 4), names=(\"loss\", \"acc\", \"f1\")),\n",
        "    ],  # None\n",
        "}\n",
        "\n",
        "# !rm -fr /content/runs\n",
        "trainer = pl.Trainer(**trainer_params)\n",
        "trainer.fit(model, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "PPrYgAIUVZIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr runs"
      ],
      "metadata": {
        "id": "OeWe62hpBTvL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -h -d 1 runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9jLEYPtDTDr",
        "outputId": "224bd174-c9f0-4b23-9783-36d529076717"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.0K\truns/Jun20_03-18-06_e99ee3abc245\n",
            "12K\truns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WYosMkuPJBeS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}